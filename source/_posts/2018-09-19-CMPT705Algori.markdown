---
layout:     post
title:      "CMPT705-Design and Analysis of Algorithms"
tags:
    - SFUcourse
---

> “Prof.  Leonid Chindelevitch teaches this course”

[Greedy Algorithm](#01) 

[Dynamic Programming](#02)

[Basic Graph Algorithms](#03)

[Graph Decompositions for Optimization](#04)

[P, NP, NP-complete, NP-hard](#05)

<p id = "01"></p>

# Greedy Algorithm

**找零问题**

从最大面值的硬币开始，每一个面值的使用贪心法。

---



**Huffman Coding**

<br />

<br />

<p id = "02"></p>

# Dynamic Programming

**找零问题**

如何traceback

**0-1背包**

有 `n` 个物品，每个物品有重量 `w` 和 价值 `u`，背包的承重容量为 `c`，放入物体使得背包中的物品价值总和最大。

使用 $D[i,j]$ 表示使用前 `i` 个物品，最大 `j` 容量获得的最大价值，状态转移方程为

![img](/images/in-post/post-blog-0-1bag.png)

code

```c++
#include<cstdio>
#include<algorithm>
using namespace std;
#define MAXN 1000
#define MAXC 100000

int weight[MAXN], value[MAXN], x[MAXN];
int dp[MAXN][MAXC];

int main(){
    freopen("*****/Test/data.in", "r", stdin);
    freopen("*****/Test/data.out", "w", stdout);
    int n, C;
    while(scanf("%d %d", &n, &C) != EOF){
        for(int i=0; i<n; ++i)    scanf("%d %d", &weight[i], &value[i]);       
        for(int i=0; i<=n; ++i){
            for(int j=0; j<=C; ++j){
                dp[i][j] = (i==0) ? 0 : dp[i-1][j];
                if(i>0 && j>=weight[i-1]) // i从0到n,第i个对应下标i-1
                    dp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i-1]]+value[i-1]);
            }
        }
        printf("%d\n", dp[n][C]);// answer

        int j=C; // backtracking
        for(int i=n; i>0; --i){
            if(dp[i][j] > dp[i-1][j]){
                x[i-1] = 1;
                j = j-weight[i-1];
            }
        }
        for(int i=0;i<n;++i) printf("%d ", x[i]);
        printf("\n");
    }
    fclose(stdin);
    fclose(stdout);
    return 0;
}
```

样例

```c++
输入：
5 10
4 9
3 6
5 1
2 4
4 9
输出：
22
1 0 0 1 1
```

dp只可以得到最大的价值是多大，如果需要知道方案的具体内容，即究竟选了哪几个物品，就需要回溯。回溯的思路也很简单，如果 $D[i,j] > D[i-1,j]$，那么说明物品 `i` 被选择了。`j` 初始化为总容量 `c`，`i` 从n循环至1.

---



**Longest common subsequence**

最长公共子序列，区别于最长公共子串。比如 `ABCD` 与 `DACB` 的最长公共子序列是AC。

两个字符串 $s$ 和 $t$，使用 $L[i,j]$ 表示 $s$ 的前 $i$ 个字符与 $t$ 的前 $j$ 个字符的最长公共子序列的长度，状态转移方程为

![img](/images/in-post/post-blog-lcsubseq.png)

```c++
#include<string>
#include<algorithm>
#include<iostream>
using namespace std;

int longestCommonSubsequence(string A, string B){
    int n = A.length();
    int m = B.length();
    int dp[n+1][m+1];
    for(int i = 1; i<=n; ++i)
        for(int j = 1; j<=m; ++j)
        {
            dp[i][j] = max(dp[i-1][j], dp[i][j-1]);
            if(A[i-1] == B[j-1])
                dp[i][j] = dp[i-1][j-1]+1;
        }
    return dp[n][m];
}

int main(){
    string A = "ABCD";
    string B = "DACB";
    cout << longestCommonSubsequence(A, B) << endl;
    return 0;
}
```

---



**Longest common substring**

最长公共子串。使用 $L[i,j]$  表示 $S_{1:i}$ 与 $T_{1:j}$ 的最长公共子串。转移方程为 

![img](/images/in-post/post-blog-lcsubstr.png)

这个方程的意思其实就是匹配两个字符串，每次使得 $S[i]==T[j]$，也就是 $S$ 与 $T$ 匹配了一部分而且以 $S[i]$ 或者 $T[j]$ 结尾，否则值为0，认为匹配中断。

![img](/images/in-post/post-blog-lcsubstrmatch.png)

```c++
int LongestCommonSubstring(string s1, string s2)
{
    int m = s1.size(), n = s2.size();
    int dp[m+1][n+1];
    int longest = 0;
    for(int i=0; i<=m; ++i)
        for(int j = 0; j<=n; ++j)
        {
            if (i == 0 || j == 0) dp[i][j] = 0;
            else if(s1[i-1] == s2[j-1])
            {
                dp[i][j] = dp[i-1][j-1]+1;
                longest = max(longest, dp[i][j]);
            }
            else dp[i][j] = 0;
        }
    return longest;
}
```

需要输出所有的最长公共子串，可以在每次dp得到最大值的时候，都记录一下$(i,j)$, 然后做一次遍历输出所有 $S$ 与 $T$ 的 `substr()` 相等的且长度等于最大长度的子串。

<u>这个问题还可以通过KMP算法解决，在之前的文章里</u> [link](http://miyunluo.com/2016/10/07/KMP/)

---



**Matrix multiplication**

两个矩阵的维度分别是 $x\*y$ 与 $y\*z$ , 那么两个矩阵相乘的时间复杂度是 $O(xyz)$. 但是由于矩阵乘法有结合律，所以计算的先后顺序也决定了复杂度不同。目标为使得总复杂度最小。

比如 $A$: $10\*20$, $B$: $20\*30$, $C$: $30\*40$，$(AB)C$ 需要32000操作，而 $A(BC)$ 为18000. 

记$C[i,j]$ 为$A_iA_{i+1}...A_{j-1}A_j$ 的最优解，转换方程为

![img](/images/in-post/post-blog-matrixmul.png)

```c++
#include <iostream>
#include <algorithm>
#include <climits>

int dp[1024][1024] = {0};

struct Matrix {
    int row;
    int column;
};

int matrixChainCost(Matrix *ms, int n) {
    for (int scale = 2; scale <= n; scale++) {
        for (int i = 0; i <= n - scale; i++) {
            int j = i + scale - 1;
            dp[i][j] = INT_MAX;
            for (int k = i; k < j; k++) {
                dp[i][j] = std::min(dp[i][j], dp[i][k] + dp[k+1][j] + (ms[i].row*ms[k].column*ms[j].column));
            }
        }
    }
    return dp[0][n - 1];
}
```

---



**Traveling Salesman Problem**

选取一个城市为起点，经过图中其余的每一个城市一次，最终回到起点，使得路径最短。

分析，假设顶点集合为 $V$，起点为 $v_1$，且先从 $v_1$ 到 $v_2$，则从 $v_2$ 出发沿着某一条路径回到 $v_1$ 经过 $V-\{v_1,v_2\}$ 每个点一次，且路径最短的那一条。

设 $D[v_k,S]$ 表示从 $v_k$ 出发，经过集合 $S$ 中所有城市一次且仅一次，最后回到出发城市 $v_1$ 的最短路径的长度。有 $D[v_1, V-\{v_1\}] = \min\limits_{2\le k\le n}\{d_{ik} + D[v_k,V-\{v_1,v_k\}]\}$ 

<br />

<br />

<p id = "03"></p>

# Basic Graph Algorithms

> Digraph 有向图

**Transitive Closure 传递闭包**

就是求解一个图的连通性，一般处理 **有向图**，得到一个连通矩阵。比如存在路径 $(u,v)$ and $(v,w)$，那么 $(u,w)$ 也是连通的。可见解决子问题 $(u,v)$ and $(v,w)$ 就可以解决上层问题 $(u,w)$。直接动态规划。

算法名字叫 "Warshall Algorithm"，时间复杂度为 $O(V^3)$。

```c++
for each edge (u,v)
	tc[u][v] = true;

for(k = 1; k<= V; k++)
    for(i = 1;i<= V; i++)
        for(j = 1;j<= V; j++)
            tc[i][j] ||= (tc[i][k] && tc[k][j]);
```

还有一种基于 $DFS$ 的做法 [link](http://www.cs.princeton.edu/courses/archive/spr03/cs226/lectures/digraph.4up.pdf)  (之前没有见过。。。)

比如现在有节点 $(u,v)$ 连通，那么标记 $(u,v)$ 连通之后，对于 $v$ 的所有子节点 $k$，也标记 $(u,k)$ 连通。这个方法使用邻接表会方便一些。

```c++
int graphTC(Graph G){
    for (every node in G) dfs(G, s, s); // 对于图内的所有节点
}

void dfs(Graph G, int s, int v){
    tc[s][v] = 1;
    for (w be v 的所有邻接节点)
        if(tc[s][w] == 0) dfs(G, s, w);
}
```

 时间复杂度为 $O(VE)$。

---



**Topological Sorting 拓扑排序**

拓扑排序在实际中的例子就是，不同事件发生是有顺序的，比如

拓扑排序针对 **有向无环图**  (**DAG**,**Directed Acyclic Graph**)，满足两个性质。1.每个顶点只出现一次，2.若图中存在一条A到B的路径，那么拓扑排序中A必然出现在B的前面。

根据这两个性质，尤其是第二个性质，可以很容易想到如下算法

1. 找到图中**入度**为0的点作为起始顶点（因为没有通向它的路径，拓扑排序中必然最先出现）
2. 删掉这个顶点，以及这个顶点的出边。
3. 寻找下一个入度为0的顶点，循环1和2，直到所有顶点都被遍历到。

这个方法似乎叫 “**Kahn’s algorithm for Topological Sorting**”。实现的时候，维护一个入度为0的点的集合 (或者队列)，每次随机取出一个顶点，所以可见拓扑排序并不唯一。此算法遍历了所有的顶点与所有的边，因此时间复杂度为 $O(V+E)$。

还有一种使用 $DFS$ 的做法。这个做法比较神奇，之前也没有见过，不是一般意义上的深度优先，沿着有向边反向走，比如当前搜索到了点 $u$，下一步搜索 $u$ 的所有前继节点 $v$，只要还存在前继节点，就还不能把 $u$ 放进拓扑序里。

```c++
function TSDFS(node u){
    for(all v that points to u){
        if(v没有被删掉)
        	TSDFS(v);
    }
    将u加入到拓扑序列中;
    删除u;
}
```

 $DFS$ 拓扑序可以从**出度**为0的顶点开始做起。 时间复杂度也是 $O(V+E)$。

---



**Strong Connectivity Algorithm 强连通**

针对有向图，如果 $u,v$ 连通，$v,u$ 也连通，则二者强连通。对于一个强连通分量，所包含的点两两可达。如果只是需要统计图中的强连通分量的个数，则可以使用 ***并查集*** 解决。如果需要找出强连通分量的点，使用 Kosaraju 或者 Tarjan 算法。时间复杂度$O(V+E)$。

---



> Shortest Path 最短路

**Single Source Shortest Paths 单源最短路径**

给定一个源点$s$，求 $s​$ 到其他各个顶点的最短路径。

经典算法 **Dijkstra Algorithm**，要求图 “没有负边” 有向或者无向。 算法步骤为

1. 将顶点分为两个集合 $A$ 和 $B$，$A$ 为已经找到最短路径的顶点集合，$B$ 为等待求最短路径的集合。初始 $A = \{s\}$, $B$ 集合中所有点到源点的距离为 $inf$.
2.  每次集合 $A$ 新加入一个点 $u$，将与 $u$ 相邻的所有点 $v$ 的距离使用 $dist(u)$ 松弛 $dist(v) = min(dist(v), dist(u) + e(u,v))$。初始情况将源点 $s$ 视为新加入的点，松弛操作为 $dist(v) = min(inf, e(s,v))$。
3. 选取 $B$ 中最小的 $dist$ 的点加入集合 $A$
4. 重复2.3. 直到 $B$ 为空

时间复杂度，使用邻接表，不优化，外层循环所有顶点，内层循环寻找 $B$ 中最短距离的点，松弛操作经过了所有的边，因此时间复杂度为 $O(V^2+E)$。

但是这个算法可以使用最小堆(heap)优化，使得找距离源点最小距离点的时间复杂度降到 $O(logN)$。如果这个图是一个稀疏图，则可以用邻接表来表示这个图，于是时间复杂度为 $O((E+V)logV)$。如果是个稠密图，比如 $E = V^2$，那么复杂度反而很高了。

对于有负边的图，可以使用 **Bellman-Ford Algorithm**。算法的思想为，对所有边进行 $V-1$ 次松弛操作。进行 $V-1$ 次操作的原因是，在一个包含 $V$ 个顶点的图中，任意两点之间的最短路径最多包含 $V-1$ 条边。对于每一条边$e(u,v)$，$dist(v) = min(dist(v), dist(u) + e(u,v))$。在 $V-1$轮操作后，如果再执行一次边集的松弛操作还可以松弛，那么就说明图中含有负环。时间复杂度为 $O(VE)$。

---



**All-pairs shortest paths 全源最短路径**

这个是单源最短路径的推广，上面两个算法可也以做，就是对每个顶点为源点做一次单源最短路径。这样Dijkstra算法时间复杂度为 $O(VE+V^2logV)$，Bellman-Ford算法时间复杂度为$O(V^2E)$。还有一个Floyd-Warshall算法，本质和求连通性的Warshall算法一样，时间复杂度为 $O(V^3)$。所以在稀疏图的时候前两个算法的复杂度低一点，但是对于稠密图，比如$E=V^2$，复杂度就比较高了。

还有一点就是，Dijkstra要求图边的权值不能为负，于是有一个 **Johnson Algorithm** 通过调整权值为负的图，使之可以使用Dijkstra算法，并且re-weight后，计算出的最短路路径依然正确。

比如这里有一个包含负边的图

![img](/images/in-post/post-blog-neggraph.png)

首先增加一个节点，连接所有的边，新边权值为0

![img](/images/in-post/post-blog-Johnson.png)

然后使用Bellman-ford算法 (因为有负边)，计算新增节点到其他节点的最短路径$h[i]$，使用公式 $w(u,v) = w(u,v)+(h[u] - h[v])$ 进行re-weight，然后移除新增节点，对其他节点使用Dijkstra算法。

主要运行时间为Dijkstra算法的时间。

对于正确性的证明，直观地解释是，re-weight之前的shortest path 在re-weight之后还是shortest path，参考这个链接[link](https://brilliant.org/wiki/johnsons-algorithm/)，也有讲到re-weight后的边都是正的。

---



**Shortest paths via matrix multiplication**



<br />

<br />

<p id = "04"></p>

# Graph Decompositions for Optimization

**Tree Decomposition**

定义可以参考 [wikipedia](https://en.wikipedia.org/wiki/Tree_decomposition)，将tree分解为若干个bags，其中bag满足如下几个性质。

1. every vertex is in at least one bag 每个顶点至少包含在一个bag里
2. every edge is included in some bag 每条边都包含在某个bag里
3. 如果 $u\in X_w$ 并且 $u\in X_z$，那么 $u$ 在两个bag通路上的所有bag里 

width of a tree decomposition 是最大的bag的大小 - 1，这里指的是某一个确定的decomposition。对于 treewide，是所有可能的decomposition中的那个最小的width。

**Maximum independent set**

首先，independent set 指的是，一个点集内，所有点两两之间都不相邻。寻找maximum independent set是$NP-hard$ 问题。但是在确定了tree-width的时候，变成了一个定参数问题，可以使用dp解决。

例子 **The Party Problem**。邀请一些人来参加party，这些人有职位上的隶属关系，可以形成一棵树。要求不能同时邀请直接下属和上司，也就是父节点和直接子节点不能同时被选择，使得所选出来的点，总权重最大。

也就是，输入为一个节点有权重的树，寻找总权重最大的independent set。

Denote $T_v$ 为以 $v$ 为根的子树，$A[v]$ 为 $T_v$ 的independent set的最大权重，$B[v]$ 为 $T_v$ 不包含根节点 $v$ 的 independent set的最大权重。记 $v$ 的子节点为 $v'_1, v'_2,...,v'_k$，有

$B[v] = \sum_{i=1}^kA[v'_i]$

$A[v] = max(B[v], w(v)+\sum_{i=1}^kB[v'_i])$。

<br />

<br />

<p id = "05"></p>

# P, NP, NP-complete, NP-hard

理解它们的定义可同时参考 [Matrix67 的博文](http://www.matrix67.com/blog/archives/105)

+ **P Problem**: 可以在polynomial时间内解决的问题

  + polynomial time 比如 $O(1), O(n^2), O(log(n))$之类

+ **NP Problem**: （nondeterministic polynomial time）对于一个问题，没有一个已知的可以快速解决问题的算法，但是如果给出问题的一个例子，可以在polynomial时间内验证它是不是这个问题的答案。比如我可能不知道一个图怎么找到Hamilton回路[经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路]，但是如果你给我一个回路，我可以很快验证它是不是Hamilton回路。

  + NP的重点是，可以在多项式时间验证一个解 （或者说，可以在多项式时间猜出一个解）
  + **P problem** 是 **NP Problem** 的一个子集（所有可以polynomial时间解决的问题肯定可以在polynomial时间内验证解）
  + 那么就有不是NP Problem的问题。比如一个图里是否**不存在**Hamilton回路，这个问题的解就无法在多项式时间内验证，因为除非你验证了所有的回路，否则无法断定不存在。
  + 定义NP Problem的目的是，因为通常情况，只有NP Problem可以找到多项式时间的算法，我们肯定不能指望一个连多项式时间都无法验证解的问题会有一个多项式时间的算法。
  + 对于$P$是不是等于$NP$，目前人们普遍认为 $P=NP$ 不成立，因为在研究NP问题的时候，找到了一类非常特殊的问题 NP-complete

+ **Reduction 归约**：一般来说是 Polynomial-Time Reduction，问题$X$可以在polynomial-time的操作内转化到问题$Y$, 记做$X\le_P Y$，意思是，如果$Y$可以在polynomial时间内解决，那么$X$也可以，直接用$Y$的算法就可以解决$X$，否则，如果$Y$不能在polynimial时间内解决，那么$X$也不行。比如Hamilto回路可以归约为TSP(Travelling Salesman Problem)问题，在Hamilton回路中，将路径上的边权值设为0，其他边权值为1，转换到TSP问题就是，是否存在一条长为0的路径。

  + $X$归约到$Y$，那么$Y$的时间复杂度$\ge X$的时间复杂度，那么不断向上归约，不断找复杂度更高但应用范围更广的算法来代替复杂度低应用范围小的算法，最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP问题的这样一个超级NP问题？答案是肯定的，就是NP-complete问题。

+ **NP-complete**：

  + 定义 1.是一个NP Problem，2.所有的NP问题可以归约到它
  + 要证一个问题是NPC，先说明它至少是一个NP问题，然后找到一个NPC问题可以归约到它
  + 于是需要第一个NPC问题，<u>第一个NPC问题来源于逻辑电路，可以证明所有的NP问题可以归约到它，对于NP问题来说，问题转化为了求出满足结果为True的一个输入</u>
  + 有了第一个NPC问题，就有了一大堆NPC问题，只要做归约就可以（比如Hamilton 回路 和TSP问题都是NPC）

  + 所有的NP可以归约到NPC，那么只要任意一个NPC找到了多项式时间的算法，那么所有的NP都解决了，NP也就等于P了

+ **NP-hard**：

  + 满足NP-complete定义的第二条，但是不一定满足第一条（即不一定是NP问题）
  + 有可能比所有的NPC的时间复杂度更高
  + 要证明一个问题是NP-hard，一般是找一个decision problem是NP-complete（通过polynomial-time reduction到另一个NP-complete问题），那么它是NP-hard（因为general case肯定比 decision problem 更复杂，也就是 如何找到一个解比判断解是否存在要更复杂）

P, NP, NP-complete, NP-hard 之间的关系如图

![img](/images/in-post/post-blog-np.png)

