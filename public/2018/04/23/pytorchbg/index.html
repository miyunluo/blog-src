<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="“防止以后踩坑”  第一眼看到pytorch就十分喜欢，就像第一眼看到golang。 pytorch 0.4 发布，api改了。。。 0.0 Tensor and Variable Tensor  1234567891011121314151617181920212223242526272829303132# 沿行取最大值max_value, max_idx = torch.max(x, dim">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch自用笔记">
<meta property="og:url" content="http://miyunluo.com/2018/04/23/pytorchbg/index.html">
<meta property="og:site_name" content="miyunLuo">
<meta property="og:description" content="“防止以后踩坑”  第一眼看到pytorch就十分喜欢，就像第一眼看到golang。 pytorch 0.4 发布，api改了。。。 0.0 Tensor and Variable Tensor  1234567891011121314151617181920212223242526272829303132# 沿行取最大值max_value, max_idx = torch.max(x, dim">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-05-06T10:35:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pytorch自用笔记">
<meta name="twitter:description" content="“防止以后踩坑”  第一眼看到pytorch就十分喜欢，就像第一眼看到golang。 pytorch 0.4 发布，api改了。。。 0.0 Tensor and Variable Tensor  1234567891011121314151617181920212223242526272829303132# 沿行取最大值max_value, max_idx = torch.max(x, dim">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-200x200.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>pytorch自用笔记</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/research/">Research</a></li>
         
          <li><a href="/about/">About</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/04/24/DNNyoutube2016/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2018/03/25/realrbl/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://miyunluo.com/2018/04/23/pytorchbg/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://miyunluo.com/2018/04/23/pytorchbg/&text=pytorch自用笔记"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://miyunluo.com/2018/04/23/pytorchbg/&is_video=false&description=pytorch自用笔记"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=pytorch自用笔记&body=Check out this article: http://miyunluo.com/2018/04/23/pytorchbg/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://miyunluo.com/2018/04/23/pytorchbg/&name=pytorch自用笔记&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-0-Tensor-and-Variable"><span class="toc-number">1.</span> <span class="toc-text">0.0 Tensor and Variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-1-自动求导"><span class="toc-number">2.</span> <span class="toc-text">0.1 自动求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-2-动态图与静态图"><span class="toc-number">3.</span> <span class="toc-text">0.2 动态图与静态图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-0-线性模型与梯度下降"><span class="toc-number">4.</span> <span class="toc-text">1.0 线性模型与梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-逻辑回归"><span class="toc-number">5.</span> <span class="toc-text">1.1 逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-MLP-Sequential-Module"><span class="toc-number">6.</span> <span class="toc-text">1.2 MLP, Sequential, Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-多分类网络"><span class="toc-number">7.</span> <span class="toc-text">1.3 多分类网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-参数初始化"><span class="toc-number">8.</span> <span class="toc-text">1.4 参数初始化</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        pytorch自用笔记
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">miyunLuo</span>
      </span>
      
    <div class="postdate">
        <time datetime="2018-04-22T16:00:00.000Z" itemprop="datePublished">2018-04-23</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/python/">python</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>“防止以后踩坑”</p>
</blockquote>
<p>第一眼看到pytorch就十分喜欢，就像第一眼看到golang。</p>
<p>pytorch 0.4 发布，api改了。。。</p>
<h3 id="0-0-Tensor-and-Variable"><a href="#0-0-Tensor-and-Variable" class="headerlink" title="0.0 Tensor and Variable"></a>0.0 Tensor and Variable</h3><ul>
<li><strong>Tensor</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿行取最大值</span></span><br><span class="line">max_value, max_idx = torch.max(x, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 沿行求和</span></span><br><span class="line">sum_x = torch.sum(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增减维度</span></span><br><span class="line">x = x.unsqueeze(<span class="number">0</span>) <span class="comment"># 在第一维增加</span></span><br><span class="line">x = x.unsqueeze(<span class="number">1</span>) <span class="comment"># 在第二维增加</span></span><br><span class="line"></span><br><span class="line">x = x.squeeze(<span class="number">0</span>)   <span class="comment"># 减少第一维度</span></span><br><span class="line">x = x.squeeze()    <span class="comment"># tensor中所有维度为1全部去掉</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度交换</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># permute 重新排列</span></span><br><span class="line">x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># x : torch.Size([4, 3, 5])</span></span><br><span class="line"><span class="comment"># transpose 交换 tensor 中两个维度</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># x : torch.Size([5, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># view 对 tensor 进行 reshape</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">x = x.view(<span class="number">-1</span>, <span class="number">5</span>) <span class="comment"># -1 表示任意大小，5 表示第二维变成 5</span></span><br><span class="line">				  <span class="comment"># x : torch.Size([12, 5])</span></span><br><span class="line">x = x.view(<span class="number">3</span>, <span class="number">20</span>) <span class="comment"># x : torch.Size([3, 20])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># inplace 操作，直接对tensor操作而不需要另外开辟内存空间。一般是操作符后加_</span></span><br><span class="line">x.unsqueeze_(<span class="number">0</span>)     <span class="comment"># unsqueeze 进行 inplace</span></span><br><span class="line">x.transpose_(<span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># transpose 进行 inplace</span></span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x.add_(y)	<span class="comment"># add 进行 inplace</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>Variable</strong></p>
<p>Variable 是对 tensor 的封装，每个 Variabel都有三个属性，Variable 中的 tensor本身<code>.data</code>，对应 tensor 的梯度<code>.grad</code>以及这个 Variable 是通过什么方式得到的<code>.grad_fn</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">y_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 tensor 变成 Variable</span></span><br><span class="line">x = Variable(x_tensor, requires_grad=<span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># 默认 Variable 是不需要求梯度的，所以申明需要对其进行求梯度</span></span><br><span class="line">y = Variable(y_tensor, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">z = torch.sum(x + y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求 x 和 y 的梯度</span></span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment"># Variable containing:</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment">#     1     1     1     1     1</span></span><br><span class="line"><span class="comment"># [torch.FloatTensor of size 10x5]</span></span><br><span class="line"></span><br><span class="line">print(y.grad)</span><br></pre></td></tr></table></figure>
<h3 id="0-1-自动求导"><a href="#0-1-自动求导" class="headerlink" title="0.1 自动求导"></a>0.1 自动求导</h3><ul>
<li><p><strong>多次自动求导</strong></p>
<p>调用 backward 后自动计算一次导数，再次调用会报错，因为pytorch默认做完一次自动求导后，计算图被丢弃，两次求导需要手动设置。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.FloatTensor([<span class="number">3</span>]), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = x * <span class="number">2</span> + x ** <span class="number">2</span> + <span class="number">3</span></span><br><span class="line"></span><br><span class="line">y.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment"># Variable containing:</span></span><br><span class="line"><span class="comment">#  8</span></span><br><span class="line"><span class="comment"># [torch.FloatTensor of size 1]</span></span><br><span class="line"></span><br><span class="line">y.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(x.grad) </span><br><span class="line"><span class="comment"># 输出 16，因为做了两次自动求导，所以将第一次的梯度 8 和第二次的梯度 8 相加得到 16</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 再做一次自动求导，这次不保留计算图</span></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment"># 输出 24</span></span><br><span class="line"></span><br><span class="line">y.backward() <span class="comment"># 再做会报错，计算图已经丢弃</span></span><br></pre></td></tr></table></figure>
<h3 id="0-2-动态图与静态图"><a href="#0-2-动态图与静态图" class="headerlink" title="0.2 动态图与静态图"></a>0.2 动态图与静态图</h3><p>pytorch与python的写法基本一致，没有任何额外的学习成本。tensorflow需要先定义图，然后执行，不能直接使用while，需要使用tf.while_loop，有些反直觉。</p>
<h3 id="1-0-线性模型与梯度下降"><a href="#1-0-线性模型与梯度下降" class="headerlink" title="1.0 线性模型与梯度下降"></a>1.0 线性模型与梯度下降</h3><p>最简单的线性模型$y=x*w+b$，计算误差函数为$\frac{1}{n}\sum^n_{i=1}(\widehat{y}_i-y_i)^2$</p>
<p>数分里都学过梯度的意义在于，沿着梯度函数变化最快，为了尽快找到误差的最小值，需要沿着梯度方向更新$w,b$，二者的梯度分别为</p>
<p>$$\frac{\partial}{\partial w}=\frac{2}{n}\sum^n_{i=1}x_i(wx_i+b-y_i)$$</p>
<p>$$\frac{\partial}{\partial b}=\frac{2}{n}\sum^n_{i=1}(wx_i+b-y_i)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">torch.manual_seed(<span class="number">2018</span>)</span><br><span class="line"><span class="comment"># 读入数据 x 和 y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据 x 和 y</span></span><br><span class="line">x_train = np.array([...], dtype=np.float32)</span><br><span class="line">y_train = np.array([...], dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换成 Tensor</span></span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line"><span class="comment"># 定义参数 w 和 b</span></span><br><span class="line">w = Variable(torch.randn(<span class="number">1</span>), requires_grad=<span class="keyword">True</span>) <span class="comment"># 随机初始化</span></span><br><span class="line">b = Variable(torch.zeros(<span class="number">1</span>), requires_grad=<span class="keyword">True</span>) <span class="comment"># 使用0进行初始化</span></span><br><span class="line"><span class="comment"># 构建线性回归模型</span></span><br><span class="line">x_train = Variable(x_train)</span><br><span class="line">y_train = Variable(y_train)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"><span class="comment"># 计算误差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(y_, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mean((y_ - y_train) ** <span class="number">2</span>)</span><br><span class="line">loss = get_loss(y_, y_train)</span><br><span class="line"><span class="comment"># 自动求导</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 查看 w 和 b 的梯度</span></span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"><span class="comment"># 更新一次参数</span></span><br><span class="line">w.data = w.data - <span class="number">1e-2</span> * w.grad.data</span><br><span class="line">b.data = b.data - <span class="number">1e-2</span> * b.grad.data</span><br></pre></td></tr></table></figure>
<h3 id="1-1-逻辑回归"><a href="#1-1-逻辑回归" class="headerlink" title="1.1 逻辑回归"></a>1.1 逻辑回归</h3><p>Logistic 回归处理的是一个分类问题 (二分类)，形式与线性回归一样，都是 $y=wx+b$，但是它使用 Sigmod 函数将结果变到 0 ~ 1 之间。对于任意输入一个数据，经过 Sigmoid 之后的结果记为 $\widehat{y}$</p>
<ul>
<li>损失函数</li>
</ul>
<p>$$loss=-(ylog(\widehat{y})+(1-y)log(1-\widehat{y}))$$</p>
<p>$y$ 表示真实 label，取值 {0, 1}。如果 $y$ 是 0，表示属于第一类，则希望 $\widehat{y}$ 越小越好，这时 $loss$ 函数为，$loss=-(log(1-\widehat{y}))$，根据函数单调性，最小化 $loss$ 也就是最小化 $\widehat{y}$，与要求一致。</p>
<p>如果 $y$ 是 1，表示属于第二类，则希望 $\widehat{y}$ 越大越好，这时 $loss$ 函数为，$loss=-(log(\widehat{y}))$，最小化 $loss$ 是最大化 $\widehat{y}$。</p>
<p>使用 <code>torch.optim</code> 更新参数，需要配合另一个数据类型 <code>nn.Parameter</code>，本质上与 Variable 一样，但是 Parameter 默认要求梯度。pytorch 也提供了 Sigmode 函数，通过导入 <code>torch.nn.functional</code> 使用。</p>
<p>pytorch 提供了 <code>nn.BCEWithLogitsLoss()</code>，将 sigmoid 和 loss 写在一层，有更快的速度与稳定性。所以使用它的话，就不需要再定义 Sigmod 函数了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不使用自带的loss</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">w = nn.Parameter(torch.randn(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">b = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 定义sigmod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> F.sigmoid(torch.mm(x, w) + b)</span><br><span class="line"><span class="comment"># 定义loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_loss</span><span class="params">(y_pred, y)</span>:</span></span><br><span class="line">    logits = (y * y_pred.clamp(<span class="number">1e-12</span>).log() + (<span class="number">1</span> - y) * (<span class="number">1</span> - y_pred).clamp(<span class="number">1e-12</span>).log()).mean()</span><br><span class="line">    <span class="keyword">return</span> -logits</span><br><span class="line"><span class="comment">#- - - - - - - - - - - - - - - - - - - -</span></span><br><span class="line">optimizer = torch.optim.SGD([w, b], lr=<span class="number">1.</span>)</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">y_pred = logistic_regression(x_data)</span><br><span class="line">loss = binary_loss(y_pred, y_data) <span class="comment"># 计算 loss</span></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># 使用优化器将梯度归 0</span></span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step() <span class="comment"># 使用优化器来更新参数</span></span><br><span class="line"><span class="comment">##########################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自带的loss</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line"><span class="comment"># 使用 torch.optim 更新参数</span></span><br><span class="line">w = nn.Parameter(torch.randn(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">b = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_reg</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(x, w) + b</span><br><span class="line"><span class="comment">#- - - - - - - - - - - - - - - - - - - -</span></span><br><span class="line">optimizer = torch.optim.SGD([w, b], lr=<span class="number">1.</span>)</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">y_pred = logistic_reg(x_data)</span><br><span class="line">loss = criterion(y_pred, y_data)</span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment"># 计算正确率</span></span><br><span class="line">mask = y_pred.ge(<span class="number">0.5</span>).float()</span><br><span class="line">acc = (mask == y_data).sum().data[<span class="number">0</span>] / y_data.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="1-2-MLP-Sequential-Module"><a href="#1-2-MLP-Sequential-Module" class="headerlink" title="1.2 MLP, Sequential, Module"></a>1.2 MLP, Sequential, Module</h3><p><strong>MLP</strong></p>
<p>神经网络使用的激活函数都是非线性的，sigmode函数 $\sigma(x)=\frac{1}{1+e^{-x}}$，tanh函数 $tanh(x)=2\sigma(2x)-1$，ReLU函数 $ReLU(x)=max(0,x)$。其中ReLU使用的较多，因为计算简单，可以加速梯度下降的收敛速度。</p>
<ul>
<li><p>为什么要使用激活函数</p>
<p>假设一个两层的网络，不使用激活函数，结构为$y=w_2(w_1x)=(w_2w_1)x=wx$，实际和一个一层的网络一样。</p>
<p>通过使用激活函数，网络可以通过改变权重实现任意形状，成为非线性分类器。</p>
</li>
</ul>
<p><strong>Sequential</strong></p>
<p>Sequential 用来构建序列化模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sequential</span></span><br><span class="line">seq_net = nn.Sequential(</span><br><span class="line">	nn.Linear(<span class="number">2</span>,<span class="number">4</span>),</span><br><span class="line">    nn.Tanh(),</span><br><span class="line">    nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 序列化模型可以通过索引访问每一层</span></span><br><span class="line">seq_net[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 打印第一层权重</span></span><br><span class="line">w0 = seq_net[<span class="number">0</span>].weight</span><br><span class="line">print(w0)</span><br><span class="line"><span class="comment"># 通过parameters获得模型参数</span></span><br><span class="line">param = seq_net.parameters()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optim = torch.optim.SGD(param, <span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
<p>保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将参数和模型保存在一起</span></span><br><span class="line">torch.save(seq_net, <span class="string">'save_seq_net.pth'</span>)</span><br><span class="line"><span class="comment"># 读取保存的模型</span></span><br><span class="line">seq_net1 = torch.load(<span class="string">'save_seq_net.pth'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只保存模型参数</span></span><br><span class="line">torch.save(seq_net.state_dict(), <span class="string">'save_seq_net_params.pth'</span>)</span><br><span class="line"><span class="comment"># 要重新读入模型的参数，首先我们需要重新定义一次模型，接着重新读入参数</span></span><br><span class="line">seq_net2 = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2</span>, <span class="number">4</span>),</span><br><span class="line">    nn.Tanh(),</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">seq_net2.load_state_dict(torch.load(<span class="string">'save_seq_net_params.pth'</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Module</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># module模板</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> 网络名字<span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, 一些定义的参数)</span>:</span></span><br><span class="line">        super(网络名字, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(num_input, num_hidden)</span><br><span class="line">        self.layer2 = nn.Sequential(...)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        定义需要用的网络层</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># 定义前向传播</span></span><br><span class="line">        x1 = self.layer1(x)</span><br><span class="line">        x2 = self.layer2(x)</span><br><span class="line">        x = x1 + x2</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 访问模型中的某层可以直接通过名字</span></span><br><span class="line">l1 = 网络名字.layer1</span><br></pre></td></tr></table></figure>
<h3 id="1-3-多分类网络"><a href="#1-3-多分类网络" class="headerlink" title="1.3 多分类网络"></a>1.3 多分类网络</h3><p><strong>softmax</strong></p>
<p>设多分类网络输出为 $z_1,z_2,…,z_k$，首先取指数变成 $e^{z_1},e^{z_2},…,e^{z_k}$，每一项都除以求和</p>
<p>$$\frac{e^{z_i}}{\sum^k_{j=1}e^{z_j}}$$</p>
<p>于是所有项求和等于1，每一项分别表示其中某一种的概率。</p>
<p><strong>交叉熵</strong></p>
<p>多分类问题使用更加复杂的loss函数，交叉熵。</p>
<p>$$cross_entropy(p,q)=E_p[-logq]=-\frac{1}{m}p(x)logq(x)$$</p>
<p>这里需要back up一下。所谓 <strong>熵</strong> 是用来反应信息量的。</p>
<ul>
<li><p>信息量</p>
<p>假设 $X$ 是一个离散随机变量，概率分布为 $p(x)=Pr(X=x)$，事件 $X=x_0$的信息量为 $I(x_0)=-log(p(x_0))$。从表达式可以看出，概率越大，信息量越小。概率越大，大家越会认为这一事件的发生几乎是确定的，这一事件的发生并不会引入太多信息量。</p>
</li>
<li><p>熵</p>
<p>假设李华考试结果为一个0-1分布，{0不及格，1及格}，根据先验知识，由于李华不好好学习，考试及格(记为事件A)概率是$p(x_A)=0.1$，不及格的概率是0.9，则所有可能结果带来的额外信息的期望为</p>
<p>$$H_A(x)=-[p(x_A)log(p(x_A))+(1-P(x_A))log(1-p(x_A))]=0.469$$</p>
<p>如果还有一个张华成绩不好不坏，考试及格(记为时间B)概率是 $p(x_B)=0.5$</p>
<p>$$H_B(x)=-[p(x_B)log(p(x_B))+(1-P(x_B))log(1-p(x_B))]=1$$</p>
<p>说明在成绩出来前，猜出张华的结果比李华要难。熵越大，变量取值越不稳定。</p>
</li>
<li><p>交叉熵</p>
<p>两个分布$p,q$，$CEH(p,q)=E_p[-logq]$</p>
</li>
</ul>
<p>对于二分类问题，loss可以写为</p>
<p>$$-\frac{1}{m}\sum^m_{i=1}(y^ilog\, sigmod(x^i)+(1-y^i)log(1-sigmod(x^i)))$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch 提供了交叉熵函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h3 id="1-4-参数初始化"><a href="#1-4-参数初始化" class="headerlink" title="1.4 参数初始化"></a>1.4 参数初始化</h3><ul>
<li><strong>使用numpy初始化</strong></li>
</ul>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/research/">Research</a></li>
         
          <li><a href="/about/">About</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-0-Tensor-and-Variable"><span class="toc-number">1.</span> <span class="toc-text">0.0 Tensor and Variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-1-自动求导"><span class="toc-number">2.</span> <span class="toc-text">0.1 自动求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-2-动态图与静态图"><span class="toc-number">3.</span> <span class="toc-text">0.2 动态图与静态图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-0-线性模型与梯度下降"><span class="toc-number">4.</span> <span class="toc-text">1.0 线性模型与梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-逻辑回归"><span class="toc-number">5.</span> <span class="toc-text">1.1 逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-MLP-Sequential-Module"><span class="toc-number">6.</span> <span class="toc-text">1.2 MLP, Sequential, Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-多分类网络"><span class="toc-number">7.</span> <span class="toc-text">1.3 多分类网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-参数初始化"><span class="toc-number">8.</span> <span class="toc-text">1.4 参数初始化</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://miyunluo.com/2018/04/23/pytorchbg/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://miyunluo.com/2018/04/23/pytorchbg/&text=pytorch自用笔记"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://miyunluo.com/2018/04/23/pytorchbg/&is_video=false&description=pytorch自用笔记"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=pytorch自用笔记&body=Check out this article: http://miyunluo.com/2018/04/23/pytorchbg/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://miyunluo.com/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://miyunluo.com/2018/04/23/pytorchbg/&name=pytorch自用笔记&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2018 Yudong Luo
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/research/">Research</a></li>
         
          <li><a href="/about/">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-102635433-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<!-- Mathjax -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


